{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_HwBNlcqzrs"
      },
      "source": [
        "### Homework 04: Optical Character Recognition\n",
        "\n",
        "Now that you have the segmented letters from the previous task, we need a way to actually convert the letters to text! You can't be bothered to just transcribe the images yourself, but you remember your professor droning on about something called MNIST and you think that these letters might be kind of similar to handwritten digits.\n",
        "\n",
        "Unfortunately, because your professor hates you, he's making you write a FFN using only numpy for the first part of this assignment. Use the dataset available from the following link for training, testing, and validation on this assignment. [Alphabet Cuttings Dataset](https://drive.google.com/drive/folders/1xK3Mp9BhXWpae-ZicfGtTqkVRW-x8ntI?usp=sharing)\n",
        "\n",
        "The code immediately below is for loading and formatting the dataset. You don't have to do anything here yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-HeldcPqzrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    REPO_URL = \"https://github.com/nd-cse-30124-fa25/cse-30124-homeworks.git\"\n",
        "    REPO_NAME = \"cse-30124-homeworks\"\n",
        "    HW_FOLDER = \"homework04\" \n",
        "\n",
        "    # Clone repo if not already present\n",
        "    if not os.path.exists(REPO_NAME):\n",
        "        !git clone {REPO_URL}\n",
        "\n",
        "    # cd into the homework folder\n",
        "    %cd {REPO_NAME}/{HW_FOLDER}\n",
        "\n",
        "except ImportError:\n",
        "    pass\n",
        "    \n",
        "def detect_rgb_contours(input_path, display=False):\n",
        "    \"\"\"\n",
        "    Detect contours in the RGB channels of a PNG image and draw all contours in hierarchy.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): Path to the input PNG image\n",
        "        line_thickness (int): Thickness of contour lines in pixels\n",
        "    \"\"\"\n",
        "    # Read the image with alpha channel\n",
        "    img = cv2.imread(input_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "    # Extract the RGB channels\n",
        "    rgb_img = img[:, :, :3]\n",
        "\n",
        "    # Convert to grayscale for contour detection\n",
        "    gray = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)\n",
        "    if display:\n",
        "        display(Image.fromarray(gray))\n",
        "\n",
        "    # Setting parameter values\n",
        "    t_lower = 50  # Lower Threshold\n",
        "    t_upper = 150  # Upper threshold\n",
        "\n",
        "    # Applying the Canny Edge filter\n",
        "    edge = cv2.Canny(gray, t_lower, t_upper)\n",
        "    # Close the edges to form complete contours\n",
        "    if display:\n",
        "        display(Image.fromarray(edge))\n",
        "\n",
        "    # Find contours recursively\n",
        "    contours, hierarchy = cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Create a separate image for each contour with different colors\n",
        "    mnist_img = np.zeros((rgb_img.shape[0], rgb_img.shape[1]), dtype=np.uint8)\n",
        "    if display:\n",
        "        print(mnist_img.shape)\n",
        "        print(len(hierarchy))\n",
        "\n",
        "        print(hierarchy)\n",
        "\n",
        "    # Generate a different color for each contour based on index\n",
        "    for i, contour in enumerate(contours):\n",
        "        if i == 1:\n",
        "            cv2.drawContours(mnist_img, [contour], -1, 0, thickness=cv2.FILLED)\n",
        "        elif i % 2 == 1:\n",
        "            cv2.drawContours(mnist_img, [contour], -1, 255, thickness=cv2.FILLED)\n",
        "\n",
        "    mnist_img = cv2.resize(mnist_img, (28, 28))\n",
        "    # Display the result with multiple contours\n",
        "    if display:\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(mnist_img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"All {len(contours)} contours with unique colors\")\n",
        "        plt.show()\n",
        "\n",
        "    return mnist_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnf5vqf8qzrt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from typing import Dict, Tuple\n",
        "from IPython.display import display\n",
        "\n",
        "def load_letter_dataset(data_dir: str, train_size: int = 7, test_size: int = 2, holdout_size: int = 1) -> Dict:\n",
        "    \"\"\"\n",
        "    Load and split letter dataset into train, test, and holdout sets.\n",
        "    \"\"\"\n",
        "    # Verify split sizes\n",
        "    assert train_size + test_size + holdout_size == 10, \"Split sizes must sum to 10\"\n",
        "\n",
        "    # Dictionary to store all instances of each letter\n",
        "    letter_instances = defaultdict(list)\n",
        "\n",
        "    # Collect all image paths\n",
        "    for filename in os.listdir(data_dir):\n",
        "        if filename.endswith('.png') and not filename[0].isdigit():\n",
        "            letter = filename[0]  # First character is the letter\n",
        "            instance_path = os.path.join(data_dir, filename)\n",
        "            letter_instances[letter].append(instance_path)\n",
        "\n",
        "    train_data = {'images': [], 'labels': []}\n",
        "    test_data = {'images': [], 'labels': []}\n",
        "    holdout_data = {'images': [], 'labels': []}\n",
        "\n",
        "    # Process each letter\n",
        "    for letter, instances in letter_instances.items():\n",
        "        # Randomly shuffle the instances\n",
        "        random.shuffle(instances)\n",
        "\n",
        "        # Split into train/test/holdout\n",
        "        train_paths = instances[:train_size]\n",
        "        test_paths = instances[train_size:train_size + test_size]\n",
        "        holdout_paths = instances[train_size + test_size:]\n",
        "\n",
        "        # Load images and add to respective sets\n",
        "        for path in train_paths:\n",
        "            img = detect_rgb_contours(path)\n",
        "            train_data['images'].append(img)\n",
        "            train_data['labels'].append(letter)\n",
        "\n",
        "        for path in test_paths:\n",
        "            img = detect_rgb_contours(path)\n",
        "            test_data['images'].append(img)\n",
        "            test_data['labels'].append(letter)\n",
        "\n",
        "        for path in holdout_paths:\n",
        "            img = detect_rgb_contours(path)\n",
        "            holdout_data['images'].append(img)\n",
        "            holdout_data['labels'].append(letter)\n",
        "\n",
        "    print(train_data['labels'][0], train_data['images'][0].shape)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(train_data['images'][0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    for dataset in [train_data, test_data, holdout_data]:\n",
        "        dataset['images'] = np.array(dataset['images'])\n",
        "        dataset['labels'] = np.array(dataset['labels'])\n",
        "\n",
        "    return {\n",
        "        'train': train_data,\n",
        "        'test': test_data,\n",
        "        'holdout': holdout_data\n",
        "    }\n",
        "\n",
        "def prepare_data(data_dict: Dict) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Prepare data for FFN training:\n",
        "    - Preprocess all images\n",
        "    - Convert labels to numerical format\n",
        "    - Split into features (X) and labels (y)\n",
        "    \"\"\"\n",
        "    # Process training data\n",
        "    X_train = np.array([img.reshape(-1) / 255 for img in data_dict['train']['images']])\n",
        "    X_test = np.array([img.reshape(-1) / 255 for img in data_dict['test']['images']])\n",
        "\n",
        "    # Convert labels to numerical format\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(data_dict['train']['labels'])\n",
        "    y_test = label_encoder.transform(data_dict['test']['labels'])\n",
        "\n",
        "    # Save label encoder mapping for reference\n",
        "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "    print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeyqaOozqzru"
      },
      "source": [
        "**Neural Network from Scratch**\n",
        "\n",
        "Your task is to implement a simple neural network from scratch in numpy to classify the letters in the dataset following the architecture shown below.\n",
        "\n",
        "In order to actually implement a training regime for our network, we'll need to specify a loss function that we can use to measure how well our network is doing. We'll use the cross entropy loss function as we're attempting a multiclass classification task.\n",
        "\n",
        "![Cross Entropy Loss](https://pbs.twimg.com/media/FBmVmdHWQAAU7gq.png)\n",
        "\n",
        "Training our network will consist of two steps primarily, forward propagation and back propagation.\n",
        "\n",
        "Forward propagation is the process of taking our input data, and passing it through the network to get a prediction.\n",
        "\n",
        "Back propagation is the process of taking the derivative of the loss function with respect to the weights and biases, and using gradient descent to update the weights and biases.\n",
        "\n",
        "![NN Training](https://raw.githubusercontent.com/SkalskiP/ILearnDeepLearning.py/e300c61fc39e480bad8d4d83616e763334b74ec7/01_mysteries_of_neural_networks/03_numpy_neural_net/supporting_visualizations/blueprint.gif)\n",
        "\n",
        "In this gif we can see a brief outline of the forward and backward propagation steps.\n",
        "\n",
        "Broadly speaking, forward is what gives us our prediction, and backward is what gives us the gradient of the loss function with respect to the weights and biases, and is how we update the weights to get closer to the right answer (done by minimizing the loss function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CuYXmZZqzru"
      },
      "source": [
        "We'll also need to implement a couple activation functions and their derivatives.\n",
        "\n",
        "We're going to be using the ReLU activation function for our hidden layers, and a softmax function for our output layer. The softmax will allow us to map our output to a probability between 0 and 1 and from there to a class based on an argmax operation.\n",
        "\n",
        "![Activation Functions](https://raw.githubusercontent.com/SkalskiP/ILearnDeepLearning.py/e300c61fc39e480bad8d4d83616e763334b74ec7/01_mysteries_of_neural_networks/03_numpy_neural_net/supporting_visualizations/activations.gif)\n",
        "\n",
        "Here we can see both activation functions and their derivatives.\n",
        "\n",
        "The part that most people find tricky about this is the backpropagation step.\n",
        "\n",
        "As we've seen in class for \"single layer\" examples, to optimize the weights of a model using gradient descent, we can rewrite the loss function in terms of the weights and then take partial derivatives with respect to each weight.\n",
        "\n",
        "![Gradient Descent](https://global.discourse-cdn.com/dlai/original/3X/f/5/f58df86a4c92695569d9536d7e752161cd0f98fb.jpeg)\n",
        "\n",
        "Will multilayer networks, how do we take the derivative of the loss function with respect to the weights, if the weights in the previous layer are reliant on the weights in the layer before them?\n",
        "\n",
        "Backpropagation is the solution to this and revolves around using the chain rule to take essentially a series of partial derivatives backwards through the network to get the gradient of the loss function with respect to the weights at each layer. We can then redistribute these gradients to update the weights of the network.\n",
        "\n",
        "![Backprop](https://miro.medium.com/v2/resize:fit:1200/0*9lo2ux8ASvt6YJkH.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CX7m-iAqzru"
      },
      "source": [
        "**BETTER TEACHING**\n",
        "\n",
        "To be honest, your best bet is to watch the youtube videos by 3Blue1Brown. He's an incredible teacher and will do a better job than I can, along with better visualizations.\n",
        "\n",
        "[![Introduction to Neural Networks](https://img.youtube.com/vi/aircAruvnKk/0.jpg)](https://www.youtube.com/watch?v=aircAruvnKk)\n",
        "\n",
        "This is an introduction to neural networks using the MNIST dataset!\n",
        "\n",
        "Then we have a great video on gradient descent.\n",
        "\n",
        "[![Gradient Descent](https://img.youtube.com/vi/IHZwWFHWa-w/0.jpg)](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
        "\n",
        "Finally I'd recommend at least his first video on backpropagation, though you should probably watch the second too.\n",
        "\n",
        "[![Backprop](https://img.youtube.com/vi/Ilg3gGewQ5U/0.jpg)](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_sCyMDgqzru"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearLayer:\n",
        "    \"\"\"\n",
        "    A fully connected (dense) layer that performs a linear transformation.\n",
        "\n",
        "    Attributes:\n",
        "        W (numpy.ndarray): Weight matrix with shape (output_dim, input_dim).\n",
        "        b (numpy.ndarray): Bias vector with shape (output_dim, 1).\n",
        "        X (numpy.ndarray): Cached input used during the forward pass.\n",
        "        dW (numpy.ndarray): Gradient with respect to the weights.\n",
        "        db (numpy.ndarray): Gradient with respect to the biases.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Initialize the LinearLayer with random weights and biases using He initialization.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Dimension of the input features.\n",
        "            output_dim (int): Number of neurons (output features).\n",
        "\n",
        "        Weight initialization:\n",
        "            Weights and biases are initialized from a normal distribution and scaled by sqrt(2/input_dim).\n",
        "        \"\"\"\n",
        "        self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2.0 / input_dim)\n",
        "        self.b = np.random.randn(output_dim, 1) * np.sqrt(2.0 / input_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute the forward pass of the linear layer.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): Input data with shape (input_dim, m) where m is the number of examples.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Linear output with shape (output_dim, m)\n",
        "\n",
        "        Notes:\n",
        "            The input X is stored for use during backpropagation.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Store the input and calculate the output of the linear layer\n",
        "        pass\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Compute the backward pass of the linear layer.\n",
        "\n",
        "        Args:\n",
        "            dA (numpy.ndarray): Gradient of the loss with respect to the output of this layer,\n",
        "                                having shape (output_dim, m).\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Gradient of the loss with respect to the input X,\n",
        "                           with shape (input_dim, m).\n",
        "\n",
        "        Updates:\n",
        "            Sets self.dW as the gradient with respect to W (shape: (output_dim, input_dim)).\n",
        "            Sets self.db as the gradient with respect to b (shape: (output_dim, 1)).\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Calculate the gradient of the loss with respect to the weights and biases\n",
        "        # TODO: Return the gradient of the loss with respect to the input\n",
        "        pass\n",
        "\n",
        "    def update(self, lr):\n",
        "        \"\"\"\n",
        "        Update the parameters of the layer using gradient descent.\n",
        "\n",
        "        Args:\n",
        "            lr (float): Learning rate for the parameter update.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Update the weights and biases of the layer using the learning rate\n",
        "        pass\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit (ReLU) activation function.\n",
        "    \"\"\"\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute the forward pass using ReLU activation.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): Input data of any shape.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Output after applying ReLU element-wise (same shape as X).\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Store the input and calculate the output of the ReLU layer\n",
        "        pass\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Compute the backward pass for the ReLU activation.\n",
        "\n",
        "        Args:\n",
        "            dA (numpy.ndarray): Gradient of the loss with respect to the ReLU output,\n",
        "                                having the same shape as the input X.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Gradient of the loss with respect to the input X.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Calculate the gradient of the loss with respect to the input\n",
        "        # TODO: Return the gradient of the loss with respect to the input\n",
        "        pass\n",
        "\n",
        "    def update(self, lr):\n",
        "        \"\"\"\n",
        "        Update function for ReLU activation. Since ReLU has no parameters, no update is performed.\n",
        "\n",
        "        Args:\n",
        "            lr (float): Learning rate\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # TODO: Update the weights and biases of the layer using the learning rate\n",
        "        pass\n",
        "\n",
        "class Softmax:\n",
        "    \"\"\"\n",
        "    Softmax activation function typically used at the output layer for multi-class classification.\n",
        "    \"\"\"\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute the forward pass using softmax activation.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): Input data with shape (n_classes, m), where n_classes is the number of classes\n",
        "                               and m is the number of examples.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Softmax probabilities with shape (n_classes, m).\n",
        "        \"\"\"\n",
        "        # TODO: Store the input and calculate the output of the softmax layer\n",
        "        pass\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Compute the backward pass for the softmax activation.\n",
        "\n",
        "        Args:\n",
        "            dA (numpy.ndarray): Gradient of the loss with respect to the softmax output,\n",
        "                                having shape (n_classes, m).\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Passed-through gradient\n",
        "\n",
        "        Note:\n",
        "            Often the derivative is combined with cross-entropy loss simplifying the gradient.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Calculate the gradient of the loss with respect to the input\n",
        "        # TODO: Return the gradient of the loss with respect to the input\n",
        "        pass\n",
        "\n",
        "    def update(self, lr):\n",
        "        \"\"\"\n",
        "        Update function for Softmax activation. No update is performed because softmax has no trainable parameters.\n",
        "\n",
        "        Args:\n",
        "            lr (float): Learning rate\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Update the weights and biases of the layer using the learning rate\n",
        "        pass\n",
        "\n",
        "class NumpyNeuralNetwork:\n",
        "    \"\"\"\n",
        "    A neural network implemented using numpy for classification tasks on MNIST-like data.\n",
        "\n",
        "    Assumed Input:\n",
        "        - X: Each column is a flattened 28x28 MNIST style image, i.e., shape (784, m) where m is the number of examples.\n",
        "\n",
        "    Example Architecture:\n",
        "        - Layer 1: Linear layer mapping from 784 to 26 features.\n",
        "        - Output Activation: Softmax.\n",
        "\n",
        "    The network supports forward propagation, backpropagation (with cross-entropy loss derivative),\n",
        "    converting probabilities to class labels, and training via mini-batch gradient descent.\n",
        "    \"\"\"\n",
        "    def __init__(self, seed=42):\n",
        "        \"\"\"\n",
        "        Initialize the neural network and its layers.\n",
        "\n",
        "        Args:\n",
        "            seed (int): Random seed for reproducibility. Default is 42.\n",
        "\n",
        "        Notes:\n",
        "            The network's weights and biases are initialized in their own init functions using He initialization.\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "        self.L1 = LinearLayer(784, 26)\n",
        "        self.softmax = Softmax()\n",
        "\n",
        "        self.layers = [self.L1, self.softmax]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the entire network.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): Input data with shape (784, m), where m is the number of examples.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Output probabilities from the network with shape (n_classes, m).\n",
        "                           Here n_classes is 26.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Calculate the output of the network\n",
        "\n",
        "        return X\n",
        "\n",
        "    def cross_entropy(self, Y_hat, Y):\n",
        "        \"\"\"\n",
        "        Compute the cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            Y_hat (numpy.ndarray): Predicted probability matrix of shape (n_classes, m).\n",
        "            Y (numpy.ndarray): One-hot encoded true labels of shape (n_classes, m).\n",
        "\n",
        "        Returns:\n",
        "            float: The average cross-entropy loss over all m examples.\n",
        "\n",
        "        Notes:\n",
        "            A small constant epsilon is added to Y_hat to avoid computing log(0).\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Calculate the cross-entropy loss\n",
        "        pass\n",
        "\n",
        "    def convert_prob_into_class(self, probs):\n",
        "        \"\"\"\n",
        "        Convert predicted probability distributions into class labels.\n",
        "\n",
        "        Args:\n",
        "            probs (numpy.ndarray): Predicted probabilities with shape (n_classes, m).\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Array of predicted class labels with shape (m,).\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Convert the probabilities into a class\n",
        "        pass\n",
        "\n",
        "    def get_accuracy(self, Y_hat, Y):\n",
        "        \"\"\"\n",
        "        Compute the classification accuracy.\n",
        "\n",
        "        Args:\n",
        "            Y_hat (numpy.ndarray): Predicted probability matrix from the network, shape (n_classes, m).\n",
        "            Y (numpy.ndarray): One-hot encoded true labels, shape (n_classes, m).\n",
        "\n",
        "        Returns:\n",
        "            float: Accuracy as a fraction between 0 and 1.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Calculate the accuracy of the network\n",
        "        pass\n",
        "\n",
        "    def backprop(self, Y_hat, Y):\n",
        "        \"\"\"\n",
        "        Perform backpropagation over the entire network to compute gradients.\n",
        "\n",
        "        Args:\n",
        "            Y_hat (numpy.ndarray): Predicted output probabilities, shape (n_classes, m).\n",
        "            Y (numpy.ndarray): One-hot encoded true labels, shape (n_classes, m).\n",
        "\n",
        "        Process:\n",
        "            Starts by computing the derivative of the cross-entropy loss with respect to the final layer\n",
        "            and then propagate the gradients backward through all layers.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Calculate the gradient of the loss with respect to the input\n",
        "        pass\n",
        "\n",
        "    def train(self, X, Y, epochs, learning_rate, batch_size=32, verbose=False):\n",
        "        \"\"\"\n",
        "        Train the neural network using mini-batch gradient descent.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): Input data with shape (784, m), where each column is a flattened MNIST style image.\n",
        "            Y (numpy.ndarray): One-hot encoded labels with shape (n_classes, m), where n_classes is 26\n",
        "            epochs (int): Number of epochs for training.\n",
        "            learning_rate (float): Learning rate for the parameter updates.\n",
        "            batch_size (int, optional): Number of examples per mini-batch. Default is 32.\n",
        "            verbose (bool, optional): If True, prints training progress every 500 epochs. Default is False.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing:\n",
        "                - 'loss_history': List of loss values for each epoch.\n",
        "                - 'accuracy_history': List of accuracy values for each epoch.\n",
        "\n",
        "        Process:\n",
        "            - Shuffles the dataset each epoch.\n",
        "            - Processes data in mini-batches.\n",
        "            - Performs a forward pass, backpropagation, and parameter updates for each mini-batch.\n",
        "            - Computes the loss and accuracy for the entire dataset after each epoch.\n",
        "        \"\"\"\n",
        "        loss_history = []\n",
        "        accuracy_history = []\n",
        "        m = X.shape[1]\n",
        "        \n",
        "        for i in range(epochs):\n",
        "            # Mini-batch processing\n",
        "            permutation = np.random.permutation(m)\n",
        "            X_shuffled = X[:, permutation]\n",
        "            Y_shuffled = Y[:, permutation]\n",
        "            \n",
        "            for j in range(0, m, batch_size):\n",
        "                X_batch = X_shuffled[:, j:j+batch_size]\n",
        "                Y_batch = Y_shuffled[:, j:j+batch_size]\n",
        "                \n",
        "                # Forward propagation\n",
        "                # TODO: Calculate the output of the network\n",
        "                \n",
        "                # Backward propagation\n",
        "                # TODO: Calculate the gradients of the loss with respect to the input\n",
        "                \n",
        "                # Update parameters\n",
        "                # TODO: Update the weights and biases of the layer using the learning rate\n",
        "            \n",
        "            # Calculate metrics for the whole epoch\n",
        "            Y_hat_full = self.forward(X)\n",
        "            loss = self.cross_entropy(Y_hat_full, Y)\n",
        "            accuracy = self.get_accuracy(Y_hat_full, Y)\n",
        "            \n",
        "            loss_history.append(loss)\n",
        "            accuracy_history.append(accuracy)\n",
        "            \n",
        "            if verbose and i % 500 == 0:\n",
        "                print(f\"Epoch {i+1}/{epochs}\")\n",
        "                print(f\"loss: {loss:.5f}\")\n",
        "                print(f\"accuracy: {accuracy:.5f}\")\n",
        "                print(\"-\" * 30)\n",
        "        \n",
        "        return {'loss_history': loss_history, 'accuracy_history': accuracy_history}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRdFPk8aqzrv"
      },
      "source": [
        "### FFN Evaluation\n",
        "\n",
        "The cell below will allow you to evaluate the performance of your FFN on the holdout set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42Z7gBmCqzrw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_on_holdout(data_dict, model):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on the holdout set\n",
        "\n",
        "    Args:\n",
        "        data_dict: Dictionary containing the dataset splits\n",
        "        model: Trained NumpyNeuralNetwork model\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy on holdout set\n",
        "        np.ndarray: Confusion matrix\n",
        "    \"\"\"\n",
        "    # Preprocess holdout data\n",
        "    X_holdout = np.array([img.reshape(-1) / 255 for img in data_dict['holdout']['images']])\n",
        "\n",
        "    # Get labels and convert to numerical format using the same encoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(data_dict['train']['labels'])  # Fit on training data to maintain same mapping\n",
        "    y_holdout = label_encoder.transform(data_dict['holdout']['labels'])\n",
        "\n",
        "    # Convert to format needed by model\n",
        "    X_holdout = X_holdout.T\n",
        "    y_holdout_onehot = np.eye(26)[y_holdout].T\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred, _ = model.full_forward_propagation(X_holdout)\n",
        "    accuracy = model.get_accuracy_value(y_pred, y_holdout_onehot)\n",
        "\n",
        "    # Get predicted classes\n",
        "    predicted_classes = np.argmax(y_pred, axis=0)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    conf_matrix = confusion_matrix(y_holdout, predicted_classes)\n",
        "\n",
        "    # Print detailed results\n",
        "    print(\"\\nHoldout Set Evaluation:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return accuracy, conf_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J0tN8V2qzrw"
      },
      "source": [
        "### Running our FNN\n",
        "\n",
        "Lets use all of our data to train and evaluate our FFN!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb0NX_Y6qzrw"
      },
      "outputs": [],
      "source": [
        "data = load_letter_dataset(\"alphabet\")\n",
        "X_train, X_test, y_train, y_test = prepare_data(data)\n",
        "\n",
        "# Convert to proper format\n",
        "X_train = X_train.T\n",
        "X_test = X_test.T\n",
        "y_train_onehot = np.eye(26)[y_train].T\n",
        "y_test_onehot = np.eye(26)[y_test].T\n",
        "\n",
        "# Initialize and train model\n",
        "model = NumpyNeuralNetwork()\n",
        "history = model.train(X_train, y_train_onehot, batch_size=32, verbose=True)\n",
        "\n",
        "# Evaluate on holdout set\n",
        "holdout_accuracy, conf_matrix = evaluate_on_holdout(data, model)\n",
        "\n",
        "# Visualize results\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['cost_history'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['accuracy_history'])\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix on Holdout Set')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEs1r9Elqzrw"
      },
      "source": [
        "### Target Accuracy: 70% on Holdout Set\n",
        "\n",
        "Instead of giving hard values, which is basically impossible in deep learning, I'll be giving you a target output accuracy instead. Your goal is to reach 70% accuracy on the holdout set. You'll almost certainly have to test a number of different combinations of architectures and hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4bMTRN5qzrw"
      },
      "source": [
        "### CNN Experiment\n",
        "\n",
        "While the FFN is okay, it's really not that well suited to image classification tasks such as this. Fighting through the hangover, you recall something about the news channel CNN? Implement a CNN (using pytorch) below and see if you can get a better result than the FFN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHOavdLHqzrw"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports for both experiments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "class LetterDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "\n",
        "        # Use LabelEncoder to encode the labels\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.labels = self.label_encoder.fit_transform(labels)  # Fit and transform labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image and label\n",
        "        image = Image.fromarray(self.images[idx], mode='L')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Apply transform to image if specified\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "#TODO: Define the neural network architecture\n",
        "class BasicCNN(nn.Module):\n",
        "    def __init__(self, num_classes=26):  # Assuming 26 classes (A-Z)):\n",
        "      \"\"\"\n",
        "        Basic CNN for letter classification.\n",
        "\n",
        "        Inputs:\n",
        "          - num_classes: Number of output classes (default: 26 for A-Z).\n",
        "\n",
        "        Output:\n",
        "          - Logits (before softmax) representing class predictions.\n",
        "      \"\"\"\n",
        "      pass\n",
        "\n",
        "    def forward(self, x):\n",
        "      \"\"\"\n",
        "        Forward pass of the CNN.\n",
        "\n",
        "        Inputs:\n",
        "          - x: Input image tensor of shape (batch_size, 1, 28, 28).\n",
        "\n",
        "        Output:\n",
        "          - Logits for classification.\n",
        "      \"\"\"\n",
        "      pass\n",
        "\n",
        "#TODO: Training function for the CNN\n",
        "def train_model(model, train_loader, val_loader, device, num_epochs=100):\n",
        "  \"\"\"\n",
        "    Trains a CNN model using mini-batch gradient descent and evaluates it on a validation set.\n",
        "\n",
        "    Inputs:\n",
        "      - model: The neural network model to be trained\n",
        "      - train_loader: DataLoader for the training dataset\n",
        "      - val_loader: DataLoader for the validation dataset\n",
        "      - device: The device (CPU or GPU) to run training on\n",
        "      - num_epochs: Number of epochs for training\n",
        "      - learning_rate: learning rate\n",
        "\n",
        "    Outputs:\n",
        "      - Dictionary containing training loss, training accuracy, and validation accuracy history\n",
        "  \"\"\"\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # TODO: Train the model\n",
        "            pass\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct / total\n",
        "\n",
        "        # TODO: Validate model on validation set\n",
        "        pass\n",
        "\n",
        "        val_acc = 100. * correct / total\n",
        "\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, '\n",
        "                f'Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQkxyr9zqzrw"
      },
      "outputs": [],
      "source": [
        "# Load data (using your existing load_letter_dataset function)\n",
        "data_dict = load_letter_dataset(\"homework_datasets/alphabet\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LetterDataset(data_dict['train']['images'],\n",
        "                                data_dict['train']['labels'],\n",
        "                                transform=transform)\n",
        "val_dataset = LetterDataset(data_dict['test']['images'],\n",
        "                            data_dict['test']['labels'],\n",
        "                            transform=transform)\n",
        "holdout_dataset = LetterDataset(data_dict['holdout']['images'],\n",
        "                                data_dict['holdout']['labels'],\n",
        "                                transform=transform)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "holdout_loader = DataLoader(holdout_dataset, batch_size=32)\n",
        "\n",
        "# Initialize model and training components\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BasicCNN().to(device)\n",
        "\n",
        "# Train model\n",
        "train_model(model, train_loader, val_loader, device)\n",
        "\n",
        "# Evaluate on holdout set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in holdout_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "holdout_acc = 100. * correct / total\n",
        "print(f'Holdout Accuracy: {holdout_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecHzhz5Mqzrx"
      },
      "source": [
        "### Target Accuracy: 80% on Holdout Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH0iEqP6qzrx"
      },
      "source": [
        "Unfortunately, despite having the text, you still can't read it. It appears to be encoded with some kind of cipher. If only there were seq2seq models that you maybe could use to decode it..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
